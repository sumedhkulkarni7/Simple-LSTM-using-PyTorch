{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into a project with a full dataset, let's just take a look at how the PyTorch LSTM layer really works in practice by visualizing the outputs. We don't need to instantiate a model to see how the layer works. (You don’t need to run on a GPU for this portion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the other kinds of layers, we can instantiate an LSTM layer and provide it with the necessary arguments. The full documentation of the accepted arguments can be found here. In this example, we will only be defining the input dimension, hidden dimension and the number of layers.\n",
    "\n",
    " - Input dimension - represents the size of the input at each time step\n",
    "    * e.g. input of dimension 5 will look like this [1, 3, 8, 2, 3]\n",
    " - Hidden dimension - represents the size of the hidden state and cell state at each time step\n",
    "    * e.g. the hidden state and cell state will both have the shape of [3, 5, 4] if the hidden dimension is 3\n",
    " - Number of layers - the number of LSTM layers stacked on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy data to see how the layer takes in the input. As our input dimension is 5, we have to create a tensor of the shape (1, 1, 5) which represents (batch size, sequence length, input dimension).\n",
    "\n",
    "Additionally, we'll have to initialize a hidden state and cell state for the LSTM as this is the first cell. The hidden state and cell state is stored in a tuple with the format (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5])\n",
      "Hidden shape: (torch.Size([1, 1, 10]), torch.Size([1, 1, 10]))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "print(\"Input shape: {}\".format(inp.shape))\n",
    "print(\"Hidden shape: ({}, {})\".format(hidden[0].shape, hidden[1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll feed the input and hidden states and see what we’ll get back from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n",
      "Hidden:  (tensor([[[ 0.1268,  0.3301,  0.0041,  0.2489, -0.2372, -0.0246, -0.5380,\n",
      "          -0.0507, -0.1683,  0.1378]]], grad_fn=<StackBackward0>), tensor([[[ 0.2830,  0.8670,  0.0084,  0.4287, -0.3780, -0.0677, -0.8299,\n",
      "          -0.0996, -0.4272,  0.4779]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process above, we saw how the LSTM cell will process the input and hidden states at each time step. However in most cases, we'll be processing the input data in large sequences. The LSTM can also take in sequences of variable length and produce an output at each time step. Let's try changing the sequence length this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the output's 2nd dimension is 3, indicating that there were 3 outputs given by the LSTM. This corresponds to the length of our input sequence. For the use cases where we'll need an output at every time step, such as Text Generation, the output of each time step can be extracted directly from the 2nd dimension and fed into a fully connected layer. For text classification tasks, such as Sentiment Analysis, the last output can be taken to be fed into a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the last output in the sequence\n",
    "out = out.squeeze()[-1, :]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell Equations\n",
    "### Input Gate\n",
    "***i<sub>1</sub> = σ(W<sub>i1</sub> ⋅ (H<sub>t−1</sub>, x<sub>t</sub>) + bias<sub>i<sub><sub>1</sub>)***\n",
    "\n",
    "***i<sub>2</sub> = tanh(W<sub>i2</sub> ⋅ (H<sub>t−1</sub>, x<sub>t</sub>) + bias<sub>i<sub><sub>2</sub>)***\n",
    "\n",
    "***i<sub>input</sub> = i<sub>1</sub> ∗ i<sub>2</sub>***\n",
    "\n",
    "### Forget Gate\n",
    "***f = σ(W<sub>forget</sub> ⋅ (H<sub>t−1</sub>, x<sub>t</sub>) + bias<sub>forget</sub>)***\n",
    "\n",
    "***C<sub>t</sub> = C<sub>t−1</sub> ∗ f + i<sub>input</sub>***\n",
    "\n",
    "### Output Gate\n",
    "***O<sub>1</sub> = σ(W<sub>output1</sub> ⋅ (H<sub>t−1</sub>, x<sub>t</sub>) + bias<sub>output1</sub>)***\n",
    "\n",
    "***O<sub>2</sub> = tanh(W<sub>output2</sub> ⋅ C<sub>t</sub> + bias<sub>output2</sub>)***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
